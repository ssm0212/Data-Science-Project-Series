{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV-WnhLnQN6y",
        "outputId": "9e76e56c-ad3b-42dc-9a05-98bde313ab59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install joblib\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "import nltk\n"
      ],
      "metadata": {
        "id": "F78KgIeYRB4r"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvKPC-EORJuY",
        "outputId": "e455efba-be11-4317-c271-ea3c26744b24"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_data = pd.read_csv('train_data.csv', encoding='latin-1', )\n",
        "test_data = pd.read_csv('test_data.csv', encoding='latin-1')\n",
        "train_data_sampled = train_data.sample(n=10000, random_state = 42)"
      ],
      "metadata": {
        "id": "WY-kifSRRNXl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Exploration\n",
        "print(train_data.head())\n",
        "print(train_data.info())\n",
        "print(train_data['sentiment'].value_counts())\n",
        "train_data.shape\n",
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKLpVQGKRQoF",
        "outputId": "01b25431-cee7-4aa8-8dfc-6a1f61004914"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment Time of Tweet Age of User  \\\n",
            "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
            "1                             Sooo SAD  negative          noon       21-30   \n",
            "2                          bullying me  negative         night       31-45   \n",
            "3                       leave me alone  negative       morning       46-60   \n",
            "4                        Sons of ****,  negative          noon       60-70   \n",
            "\n",
            "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
            "0  Afghanistan          38928346         652860.0               60  \n",
            "1      Albania           2877797          27400.0              105  \n",
            "2      Algeria          43851044        2381740.0               18  \n",
            "3      Andorra             77265            470.0              164  \n",
            "4       Angola          32866272        1246700.0               26  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27480 entries, 0 to 27479\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   textID            27480 non-null  object \n",
            " 1   text              27479 non-null  object \n",
            " 2   selected_text     27479 non-null  object \n",
            " 3   sentiment         27480 non-null  object \n",
            " 4   Time of Tweet     27480 non-null  object \n",
            " 5   Age of User       27480 non-null  object \n",
            " 6   Country           27480 non-null  object \n",
            " 7   Population -2020  27480 non-null  int64  \n",
            " 8   Land Area (Km²)   27480 non-null  float64\n",
            " 9   Density (P/Km²)   27480 non-null  int64  \n",
            "dtypes: float64(1), int64(2), object(7)\n",
            "memory usage: 2.1+ MB\n",
            "None\n",
            "sentiment\n",
            "neutral     11118\n",
            "positive     8582\n",
            "negative     7780\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4815, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clean-text\n",
        "from cleantext import clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj-r3RJASMas",
        "outputId": "e7e55dc3-6d12-4ef5-fb41-6305ddabe6b5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: clean-text in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from clean-text) (1.7.0)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from clean-text) (6.2.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import contractions"
      ],
      "metadata": {
        "id": "w_fldjgLUIgt",
        "outputId": "317726b0-7b45-42a4-b1df-ce6fa36034f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "import emoji"
      ],
      "metadata": {
        "id": "1sPD_dYUWnIl",
        "outputId": "6d033a65-0861-4c09-90af-0e775456a756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (1.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "def preprocess_text_advanced(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # Expand contractions\n",
        "    text = contractions.fix(text)\n",
        "    # Clean text using clean-text library\n",
        "    text = clean(text,\n",
        "                 lower=True,\n",
        "                 no_line_breaks=True,\n",
        "                 no_urls=True,\n",
        "                 no_emails=True,\n",
        "                 no_phone_numbers=True,\n",
        "                 no_numbers=True,\n",
        "                 no_digits=True,\n",
        "                 no_currency_symbols=True,\n",
        "                 no_punct=True,\n",
        "                 replace_with_punct=\"\",\n",
        "                 replace_with_url=\"<URL>\",\n",
        "                 replace_with_email=\"<EMAIL>\",\n",
        "                 replace_with_phone_number=\"<PHONE>\",\n",
        "                 replace_with_number=\"<NUMBER>\",\n",
        "                 replace_with_digit=\"0\",\n",
        "                 replace_with_currency_symbol=\"<CUR>\",\n",
        "                 lang=\"en\")\n",
        "    # Remove emojis\n",
        "    text = emoji.demojize(text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Convert all entries to strings and fill missing values\n",
        "train_data_sampled['text'] = train_data_sampled['text'].astype(str).fillna('')\n",
        "test_data['text'] = test_data['text'].astype(str).fillna('')\n",
        "\n",
        "train_data_sampled['cleaned_text'] = train_data_sampled['text'].apply(preprocess_text_advanced)\n",
        "test_data['cleaned_text'] = test_data['text'].apply(preprocess_text_advanced)"
      ],
      "metadata": {
        "id": "OWbnaqrdSRoR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Fill missing values in both train and test data before encoding\n",
        "train_data_sampled['sentiment'] = train_data_sampled['sentiment'].fillna(method='ffill')\n",
        "test_data['sentiment'] = test_data['sentiment'].fillna(method='ffill')\n",
        "\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_data_sampled['cleaned_text'])\n",
        "X_train_seq = tokenizer.texts_to_sequences(train_data_sampled['cleaned_text'])\n",
        "X_test_seq = tokenizer.texts_to_sequences(test_data['cleaned_text'])\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = 100\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(train_data_sampled['sentiment'])\n",
        "y_test_encoded = le.transform(test_data['sentiment'])\n",
        "\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_test_categorical = to_categorical(y_test_encoded)\n"
      ],
      "metadata": {
        "id": "CYH4vCxGSkDo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
      ],
      "metadata": {
        "id": "e3TYuPUtaF3F"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LSTM model\n",
        "embedding_dim = 100\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(len(le.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "XIXYlWf4XJ_b"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train_padded, y_train_categorical, epochs=5, batch_size=64, validation_split=0.1, verbose=2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_padded, y_test_categorical, verbose=2)\n",
        "print(\"LSTM Model Accuracy: \", accuracy)\n",
        "\n",
        "# Make predictions\n",
        "lstm_pred = model.predict(X_test_padded)\n",
        "lstm_pred_classes = np.argmax(lstm_pred, axis=1)\n"
      ],
      "metadata": {
        "id": "O1_skiJ0Z-vM",
        "outputId": "24f6efe8-d01b-4d05-e8d7-38602356b7f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "141/141 - 50s - loss: 0.9612 - accuracy: 0.5277 - val_loss: 0.8084 - val_accuracy: 0.6390 - 50s/epoch - 354ms/step\n",
            "Epoch 2/5\n",
            "141/141 - 44s - loss: 0.6815 - accuracy: 0.7162 - val_loss: 0.7650 - val_accuracy: 0.6690 - 44s/epoch - 311ms/step\n",
            "Epoch 3/5\n",
            "141/141 - 44s - loss: 0.5453 - accuracy: 0.7888 - val_loss: 0.7880 - val_accuracy: 0.6800 - 44s/epoch - 314ms/step\n",
            "Epoch 4/5\n",
            "141/141 - 44s - loss: 0.4564 - accuracy: 0.8313 - val_loss: 0.8681 - val_accuracy: 0.6680 - 44s/epoch - 312ms/step\n",
            "Epoch 5/5\n",
            "141/141 - 45s - loss: 0.3905 - accuracy: 0.8614 - val_loss: 0.9355 - val_accuracy: 0.6490 - 45s/epoch - 317ms/step\n",
            "151/151 - 4s - loss: 0.8603 - accuracy: 0.7410 - 4s/epoch - 30ms/step\n",
            "LSTM Model Accuracy:  0.7410176396369934\n",
            "151/151 [==============================] - 7s 43ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation Metrics\n",
        "def print_evaluation_metrics(y_true, y_pred, model_name):\n",
        "    y_true = y_true.astype(str)\n",
        "    y_pred = y_pred.astype(str)\n",
        "\n",
        "    print(f\"Evaluation Metrics for {model_name}:\")\n",
        "    print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision: \", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall: \", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score: \", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "print_evaluation_metrics(y_test_encoded, lstm_pred_classes, \"LSTM Model\")"
      ],
      "metadata": {
        "id": "E1TyK1BraP8M",
        "outputId": "dae93c25-f9d6-41ba-ce73-73d387fc51e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics for LSTM Model:\n",
            "Accuracy:  0.7410176531671859\n",
            "Precision:  0.7502011262407671\n",
            "Recall:  0.7410176531671859\n",
            "F1 Score:  0.744557045265264\n",
            "Confusion Matrix: \n",
            " [[ 693  262   46]\n",
            " [ 363  879  188]\n",
            " [  84  304 1996]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TOPlgtWbZwD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}